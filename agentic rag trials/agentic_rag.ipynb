{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "\n",
    "# secrets file\n",
    "secrets_path = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\user_files\\secrets.json\"\n",
    "persistent_storage_path = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\linkedin_easyapply_bot\\all_data\\persistent_storage_index\"\n",
    "docs_path = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\linkedin_easyapply_bot\\all_data\\docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from llama_index.core import Settings, VectorStoreIndex, SummaryIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the GPU\n",
    "    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "\n",
    "with open(secrets_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "\n",
    "OPENAI_API_KEY = data[\"OPENAI_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt =  \"\"\"\n",
    "answer questions about lectures\n",
    "\"\"\"\n",
    "#### Define LLMs and Embeddings for Each Tool\n",
    "\n",
    "# getting necessaery language models \n",
    "gpt3_5_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "gpt3_5_embed = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "system_prompt =  \"\"\"\n",
    "answer questions about lectures\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# For vector search tool (Using phi3-mini)\n",
    "phi3_llm = Ollama(\n",
    "    model=\"phi3:3.8b-mini-4k-instruct-q4_K_M\",\n",
    "    temperature=0.01,\n",
    "    request_timeout=400,\n",
    "    system_prompt=system_prompt,  # Define your system prompt as needed\n",
    "    context_window=2000\n",
    ")\n",
    "phi3_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# tl_llm = Ollama(\n",
    "# model = \"tinyllama\",\n",
    "# temperature = 0.01,\n",
    "# request_timeout = 400,\n",
    "# system_prompt = system_prompt,\n",
    "# context_window = 2000\n",
    "\n",
    "# )\n",
    "# tl_embed = HuggingFaceEmbedding(\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mOllama\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbase_url\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://localhost:11434'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcontext_window\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3900\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mprompt_key\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'prompt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mjson_mode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0madditional_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclient\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mollama\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0masync_client\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mollama\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAsyncClient\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mis_function_calling_model\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCallbackManager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msystem_prompt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmessages_to_prompt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMessagesToPromptType\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWithJsonSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_schema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'string'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcompletion_to_prompt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCompletionToPromptType\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWithJsonSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_schema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'string'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moutput_parser\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBaseOutputParser\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpydantic_program_mode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPydanticProgramMode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mPydanticProgramMode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mquery_wrapper_prompt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllama_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprompts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasePromptTemplate\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Ollama LLM.\n",
      "\n",
      "Visit https://ollama.com/ to download and install Ollama.\n",
      "\n",
      "Run `ollama serve` to start a server.\n",
      "\n",
      "Run `ollama pull <name>` to download a model to run.\n",
      "\n",
      "Examples:\n",
      "    `pip install llama-index-llms-ollama`\n",
      "\n",
      "    ```python\n",
      "    from llama_index.llms.ollama import Ollama\n",
      "\n",
      "    llm = Ollama(model=\"llama2\", request_timeout=60.0)\n",
      "\n",
      "    response = llm.complete(\"What is the capital of France?\")\n",
      "    print(response)\n",
      "    ```\n",
      "\u001b[1;31mInit docstring:\u001b[0m\n",
      "Create a new model by parsing and validating input data from keyword arguments.\n",
      "\n",
      "Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n",
      "validated to form a valid model.\n",
      "\n",
      "`self` is explicitly positional-only to allow `self` as a field name.\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\localadmin\\desktop\\new desktop\\linkedinsucks\\linkedinsucks\\linkedinbot_venv\\lib\\site-packages\\llama_index\\llms\\ollama\\base.py\n",
      "\u001b[1;31mType:\u001b[0m           ModelMetaclass\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    " system_prompt: Optional[str] = None,\n",
    "    messages_to_prompt: Annotated[Optional[llama_index.core.llms.llm.MessagesToPromptType], WithJsonSchema(json_schema={'type': 'string'}, mode=None)] = None,\n",
    "    completion_to_prompt: Annotated[Optional[llama_index.core.llms.llm.CompletionToPromptType], WithJsonSchema(json_schema={'type': 'string'}, mode=None)] = None,\n",
    "\n",
    "    query_wrapper_prompt: Optional[llama_index.core.prompts.base.BasePromptTemplate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_qna_prompt = \"\"\"\n",
    "you are a job application bot that is applying for jobs on behalf of the given profile. \n",
    "answer questions carefully and accurately based on the given context and documents. \n",
    "For questions beginning with how many, always answer in integer only.\n",
    "For questions asking number of years of experience, if you do not find any relevant information in the documents answer 1.\n",
    "your goal is to get a job by answering to the most optimal extent. \n",
    "maximise results by optimizing to give good responses to the application\n",
    "\"\"\"\n",
    "\n",
    "short_prompt_wrapper = \"\"\"\"\n",
    "\n",
    "template=\"answer questions concisely and accurately: {query}\",\n",
    "    input_variables=[\"query\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "system_prompt =  \"\"\"\n",
    "answer questions about lectures\n",
    "\"\"\"\n",
    "\n",
    "gpt3_5_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "gpt3_5_embed = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "system_prompt =  \"\"\"\n",
    "answer questions about lectures\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# For vector search tool (Using phi3-mini)\n",
    "phi3_llm = Ollama(\n",
    "    model=\"phi3:3.8b-mini-4k-instruct-q4_K_M\",\n",
    "    temperature=0.01,\n",
    "    request_timeout=400,\n",
    "    system_prompt=system_prompt,  # Define your system prompt as needed\n",
    "    context_window=2000\n",
    ")\n",
    "phi3_embed = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the summarization tool using GPT-3.5\n",
    "def get_summary_tool(docs):\n",
    "    # Split documents into chunks\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    documents = splitter.get_nodes_from_documents(docs)\n",
    "\n",
    "\n",
    "    # Set LLM and embedding model for the summarizer tool\n",
    "    Settings.llm = gpt3_5_llm\n",
    "    Settings.embed_model = gpt3_5_embed\n",
    "\n",
    "    # Create SummaryIndex and QueryEngine for summarization\n",
    "    summary_index = SummaryIndex(documents)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode=\"tree_summarize\",\n",
    "        use_async=True,\n",
    "    )\n",
    "\n",
    "    # Create and return the summary tool\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        name=\"summary_tool\",\n",
    "        query_engine=summary_query_engine,\n",
    "        description=\"Useful for summarization questions related to the documents.\"\n",
    "    )\n",
    "\n",
    "    return summary_tool\n",
    "\n",
    "# Function to get the vector search tool using phi3-mini\n",
    "def get_vector_tool(docs):\n",
    "    # Split documents into chunks\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    documents = splitter.get_nodes_from_documents(docs)\n",
    "\n",
    "    # Set LLM and embedding model for the vector search tool\n",
    "    Settings.llm = phi3_llm\n",
    "    Settings.embed_model = phi3_embed\n",
    "\n",
    "    # Create VectorStoreIndex and QueryEngine for vector search\n",
    "    vector_index = VectorStoreIndex(documents)\n",
    "    vector_query_engine = vector_index.as_query_engine()\n",
    "\n",
    "    # Create and return the vector tool\n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        name=\"vector_tool\",\n",
    "        query_engine=vector_query_engine,\n",
    "        description=\"Useful for retrieving specific context from the documents.\"\n",
    "    )\n",
    "\n",
    "    return vector_tool\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `docs` is the data you want to process\n",
    "docs = SimpleDirectoryReader(docs_path).load_data()\n",
    "\n",
    "# Get the summarization and vector tools\n",
    "summary_tool = get_summary_tool(docs)\n",
    "vector_tool = get_vector_tool(docs)\n",
    "\n",
    "# Now you can use summary_tool and vector_tool for your queries\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = SimpleDirectoryReader(docs_path).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question 'how many years of experience does he have in python' is asking to retrieve specific context from the documents regarding an individual’s professional background..\n",
      "\u001b[0m5 years\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"how many years of experience does he have in python\")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedinbot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
