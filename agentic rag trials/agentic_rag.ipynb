{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths \n",
    "\n",
    "# secrets file\n",
    "secrets_path = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\user_files\\secrets.json\"\n",
    "persistent_storage_path = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\linkedin_easyapply_bot\\all_data\\persistent_storage_index\"\n",
    "docs_path = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\linkedin_easyapply_bot\\all_data\\docs\"\n",
    "\n",
    "PERSIST_DIR = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\linkedin_easyapply_bot\\all_data\\persistent_storage\"\n",
    "vector_index_dir = PERSIST_DIR + r\"\\vector\"\n",
    "summary_index_dir = PERSIST_DIR + r\"\\summary\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from llama_index.core import Settings, VectorStoreIndex, SummaryIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader, PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the GPU\n",
    "    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "\n",
    "with open(secrets_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "\n",
    "OPENAI_API_KEY = data[\"OPENAI_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_persist(storage_path):\n",
    "    return os.path.exists(storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_5_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "gpt3_5_embed = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "# System prompt for the LLM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "answer questions about a candidate's resume and profile\n",
    "\"\"\"\n",
    "phi3_llm = Ollama(\n",
    "    model=\"phi3:3.8b-mini-4k-instruct-q4_K_M\",\n",
    "    temperature=0.01,\n",
    "    request_timeout=400,\n",
    "    system_prompt=system_prompt,\n",
    "    context_window=2000\n",
    ")\n",
    "\n",
    "phi3_embed = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_tmpl_str_gpt4 = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge,\n",
    "it is very important that you follow the instructions clearly and answer based on the given information.\n",
    "Answer the query in the format requested in the query.\n",
    "Do not leave blanks in the answers.\n",
    "Always answer the questions in the form of a cover letter and in first person.\n",
    "If asked for a cover letter, write a short cover letter talking about your previous work experience and how it would make you a good fit for the given role.\n",
    "If asked about why you want to work at a certain company, write a concise cover letter including the company's name and talking about your previous work experience and how it would make you a good fit for the given role.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_tmpl_str_phi3 = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge,\n",
    "it is very important that you follow the instructions clearly and answer based on the given information.\n",
    "Answer the query in the format requested in the query.\n",
    "If there are options in the query, answer by choosing one or more options as required.\n",
    "Try to read the document thoroughly to extract more info from the document.\n",
    "When asked for city, return the city name along with the state.\n",
    "Return only one answer, do not return multiple answers or list of answers unless specified.\n",
    "For queries which ask for years of experience, always return values greater than 1.\n",
    "For queries like \"how many years of experience do you have with some tool\", return just the integer.\n",
    "For queries like \"how many years of experience\", the answer should always be an integer.\n",
    "For questions that start like \"do you have experience with\", always return \"Yes\".\n",
    "For queries that begin with \"Experience with\", they are asking the number of years of experience; treat this query the same as those that ask for the number of years of experience with a certain tool.\n",
    "For queries that ask \"are you willing to relocate\" or \"are you local to a certain place\", always answer \"Yes\".\n",
    "Keep the answers concise and to the point, do not answer long sentences unless necessary or specified.\n",
    "Keep the answers concise. Answer in one or two words wherever possible. Keep the answers short, do not elaborate unless necessary, do not explain or elaborate.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from llama_index.core import Settings, VectorStoreIndex, SummaryIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader, PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the GPU\n",
    "    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "\n",
    "with open(secrets_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "\n",
    "OPENAI_API_KEY = data[\"OPENAI_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "\n",
    "def check_persist(storage_path):\n",
    "    return os.path.exists(storage_path)\n",
    "\n",
    "\n",
    "\n",
    "gpt3_5_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "gpt3_5_embed = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "# System prompt for the LLM\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "answer questions about a candidate's resume and profile\n",
    "\"\"\"\n",
    "phi3_llm = Ollama(\n",
    "    model=\"phi3:3.8b-mini-4k-instruct-q4_K_M\",\n",
    "    temperature=0.01,\n",
    "    request_timeout=400,\n",
    "    system_prompt=system_prompt,\n",
    "    context_window=2000\n",
    ")\n",
    "\n",
    "phi3_embed = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "qa_prompt_tmpl_str_gpt4 = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge,\n",
    "it is very important that you follow the instructions clearly and answer based on the given information.\n",
    "Answer the query in the format requested in the query.\n",
    "Do not leave blanks in the answers.\n",
    "Always answer the questions in the form of a cover letter and in first person.\n",
    "If asked for a cover letter, write a short cover letter talking about your previous work experience and how it would make you a good fit for the given role.\n",
    "If asked about why you want to work at a certain company, write a concise cover letter including the company's name and talking about your previous work experience and how it would make you a good fit for the given role.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_tmpl_str_phi3 = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge,\n",
    "it is very important that you follow the instructions clearly and answer based on the given information.\n",
    "Answer the query in the format requested in the query.\n",
    "If there are options in the query, answer by choosing one or more options as required.\n",
    "Try to read the document thoroughly to extract more info from the document.\n",
    "When asked for city, return the city name along with the state.\n",
    "Return only one answer, do not return multiple answers or list of answers unless specified.\n",
    "For queries which ask for years of experience, always return values greater than 1.\n",
    "For queries like \"how many years of experience do you have with some tool\", return just the integer.\n",
    "For queries like \"how many years of experience\", the answer should always be an integer.\n",
    "For questions that start like \"do you have experience with\", always return \"Yes\".\n",
    "For queries that begin with \"Experience with\", they are asking the number of years of experience; treat this query the same as those that ask for the number of years of experience with a certain tool.\n",
    "For queries that ask \"are you willing to relocate\" or \"are you local to a certain place\", always answer \"Yes\".\n",
    "Keep the answers concise and to the point, do not answer long sentences unless necessary or specified.\n",
    "Keep the answers concise. Answer in one or two words wherever possible. Keep the answers short, do not elaborate unless necessary, do not explain or elaborate.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\"\n",
    "\n",
    "# Function to get the summarization tool using GPT-3.5\n",
    "def get_summary_tool(docs):\n",
    "    # Split documents into chunks\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    documents = splitter.get_nodes_from_documents(docs)\n",
    "\n",
    "\n",
    "    # Set LLM and embedding model for the summarizer tool\n",
    "    Settings.llm = gpt3_5_llm\n",
    "    Settings.embed_model = gpt3_5_embed\n",
    "\n",
    "    # Create SummaryIndex and QueryEngine for summarization\n",
    "    # summary_index = SummaryIndex(documents)\n",
    "    \n",
    "    if check_persist(summary_index_dir):\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=summary_index_dir)\n",
    "        summary_index = load_index_from_storage(storage_context)\n",
    "    else:\n",
    "        summary_index = VectorStoreIndex(documents)\n",
    "        summary_index.storage_context.persist(summary_index_dir)\n",
    "        \n",
    "\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "            response_mode=\"refine\",\n",
    "            use_async=True,\n",
    "        )\n",
    "\n",
    "    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str_phi3)\n",
    "\n",
    "    # Update query engine prompts\n",
    "    summary_query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "\n",
    "\n",
    "    # Create and return the summary tool\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        name=\"coverletter_tool\",\n",
    "        query_engine=summary_query_engine,\n",
    "        description=\"for long answer and summary based questions about the profile\"\n",
    "    )\n",
    "\n",
    "    return summary_tool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the vector search tool using phi3-mini\n",
    "def get_vector_tool(docs):\n",
    "    # Split documents into chunks\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    documents = splitter.get_nodes_from_documents(docs)\n",
    "\n",
    "    # Set LLM and embedding model for the vector search tool\n",
    "    Settings.llm = phi3_llm\n",
    "    Settings.embed_model = phi3_embed\n",
    "\n",
    "    # Create VectorStoreIndex and QueryEngine for vector search\n",
    "    \n",
    "    if check_persist(vector_index_dir):\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=vector_index_dir)\n",
    "        vector_index = load_index_from_storage(storage_context)\n",
    "    else:\n",
    "        vector_index = VectorStoreIndex(documents)\n",
    "        vector_index.storage_context.persist(vector_index_dir)\n",
    "\n",
    "    vector_query_engine = vector_index.as_query_engine(response_mode = 'compact' , use_async = True)\n",
    "        \n",
    "    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str_phi3)\n",
    "\n",
    "    # Update query engine prompts\n",
    "    vector_query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})\n",
    "\n",
    "    # Create and return the vector tool\n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        name=\"vector_tool\",\n",
    "        query_engine=vector_query_engine,\n",
    "        description=\"Useful for retrieving specific context from the documents.\"\n",
    "    )\n",
    "\n",
    "    return vector_tool\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `docs` is the data you want to process\n",
    "docs = SimpleDirectoryReader(docs_path).load_data()\n",
    "\n",
    "# Get the summarization and vector tools\n",
    "summary_tool = get_summary_tool(docs)\n",
    "vector_tool = get_vector_tool(docs)\n",
    "\n",
    "# Now you can use summary_tool and vector_tool for your queries\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "response = query_engine.query(\"how many years of experience does he have in python\")\n",
    "print(str(response.response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "response = query_engine.query(\"how many years of experience does he have in python\")\n",
    "print(str(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 2024\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"when did you graduate\")\n",
    "print(str(response.response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedinbot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
