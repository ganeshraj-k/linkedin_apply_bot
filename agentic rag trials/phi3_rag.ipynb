{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1650\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader, PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Path to the documents\n",
    "docs_path = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\linkedin_easyapply_bot\\all_data\\docs\"\n",
    "documents = SimpleDirectoryReader(docs_path).load_data()\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the GPU\n",
    "    gpu_name = torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "    print(f\"Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Load documents\n",
    "\n",
    "\n",
    "# System prompt for the LLM\n",
    "system_prompt = \"\"\"\n",
    "answer questions about a candidate's resume and profile\n",
    "\"\"\"\n",
    "phi3_llm = Ollama(\n",
    "    model=\"phi3:3.8b-mini-4k-instruct-q4_K_M\",\n",
    "    temperature=0.01,\n",
    "    request_timeout=400,\n",
    "    system_prompt=system_prompt,\n",
    "    context_window=2000\n",
    ")\n",
    "\n",
    "phi3_embed = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "# Configure LLM settings\n",
    "Settings.llm = phi3_llm\n",
    "\n",
    "# Configure embedding model\n",
    "Settings.embed_model = phi3_embed\n",
    "\n",
    "# Create an index from documents\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create a query engine\n",
    "query_engine = index.as_query_engine(response_mode=\"compact\")\n",
    "\n",
    "# Define the prompt template\n",
    "qa_prompt_tmpl_str = \"\"\"\\\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge,\n",
    "it is very important that you follow the instructions clearly and answer based on the given information.\n",
    "Answer the query in the format requested in the query.\n",
    "If there are options in the query, answer by choosing one or more options as required.\n",
    "Try to read the document thoroughly to extract more info from the document.\n",
    "When asked for city, return the city name along with the state.\n",
    "Return only one answer, do not return multiple answers or list of answers unless specified.\n",
    "For queries which ask for years of experience, always return values greater than 1.\n",
    "For queries like \"how many years of experience do you have with some tool\", return just the integer.\n",
    "For queries like \"how many years of experience\", the answer should always be an integer.\n",
    "For questions that start like \"do you have experience with\", always return \"Yes\".\n",
    "For queries that begin with \"Experience with\", they are asking the number of years of experience; treat this query the same as those that ask for the number of years of experience with a certain tool.\n",
    "For queries that ask \"are you willing to relocate\" or \"are you local to a certain place\", always answer \"Yes\".\n",
    "Keep the answers concise and to the point, do not answer long sentences unless necessary or specified.\n",
    "Keep the answers concise. Answer in one or two words wherever possible. Keep the answers short, do not elaborate unless necessary, do not explain or elaborate.\n",
    "\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
    "\n",
    "# Update query engine prompts\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "# Example query\n",
    "response = query_engine.query(\"how many years do you have in SQL\")\n",
    "print(response.response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import nest_asyncio\n",
    "# from llama_index.core import VectorStoreIndex\n",
    "# from llama_index.core.schema import TextNode\n",
    "\n",
    "# # Apply nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Path to the JSON file\n",
    "# questions_json_path = r\"C:\\Users\\localadmin\\Desktop\\new desktop\\linkedinsucks\\linkedin_easyapply_bot\\all_data\\docs\\questions.json\"\n",
    "\n",
    "# # Load JSON data and create TextNode objects\n",
    "# few_shot_nodes = []\n",
    "# for line in open(questions_json_path, \"r\"):\n",
    "#     few_shot_nodes.append(TextNode(text=line))\n",
    "\n",
    "# # Create a VectorStoreIndex and retriever\n",
    "# few_shot_index = VectorStoreIndex(few_shot_nodes)\n",
    "# few_shot_retriever = few_shot_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# # Function to extract key-value pairs from a string\n",
    "# def extract_key_value_from_string(input_string):\n",
    "#     # Remove any trailing commas and newline characters\n",
    "#     cleaned_string = input_string.strip().rstrip(',\\n}{')\n",
    "#     if ':' not in cleaned_string:\n",
    "#         return \"\", \"\"\n",
    "#     # Split the string into key and value\n",
    "#     key, value = cleaned_string.split('\": ')\n",
    "    \n",
    "#     # Remove any surrounding quotes from key and value\n",
    "#     key = key.strip('\"')\n",
    "#     value = value.strip('\"')\n",
    "    \n",
    "#     return key, value\n",
    "\n",
    "# # Function to generate few-shot examples\n",
    "# def few_shot_examples_fn(**kwargs):\n",
    "#     query_str = kwargs[\"query_str\"]\n",
    "#     retrieved_nodes = few_shot_retriever.retrieve(query_str)\n",
    "#     result_strs = []\n",
    "#     for n in retrieved_nodes:\n",
    "#         query, response = extract_key_value_from_string(n.get_content())\n",
    "#         result_str = f\"\"\"\\\n",
    "# Query: {query}\n",
    "# Response: {response}\"\"\"\n",
    "#         result_strs.append(result_str)\n",
    "#     return \"\\n\\n\".join(result_strs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write prompt template with functions\n",
    "# qa_prompt_tmpl_str = \"\"\"\\\n",
    "# Context information is below.\n",
    "# ---------------------\n",
    "# {context_str}\n",
    "# ---------------------\n",
    "# \"Given the context information and not prior knowledge, \"\n",
    "# \"IT is very important that you         follow the instructions clearly and answer based on the given information\"\n",
    "# \"answer the query in the format requested in the query. \"\n",
    "# \"If there are options in the query, answer by choosing one or more options as required. \"\n",
    "# \"try to read the document thoroughly to extract more info from the document  \"\n",
    "# \"when asked for city return the city name along with the state\"\n",
    "# \"return only one answer, do not return multiple answers or list of answers unless specified\"\n",
    "# \"for queries which ask for years of experience always return values greater than 1\"\n",
    "# \"for queries like how many years of experience do you have with some tool, return just the integer\"\n",
    "# \"for queries like how many years of experience the answer should always be an integer\"\n",
    "# \"for questions that start like do you have experience with always return Yes\"\n",
    "# \"for queries that begin with Experience with  they are asking the number of years of experience, treat this query same as those that ask for number of years of experience with a certain tool\"\n",
    "# \"for queries that ask are you willing to relocate or are you local to a certain place always answer yes\"\n",
    "# \"Keep the answers concise and to the point, do not answer long sentences unless necessary or specified. \"\n",
    "# \"Keep the answers concise. Answer in one or two words wherever possible. Keep the answers short, do not elaborate unless necessary, do not explain or elaborate\"\n",
    "\n",
    "# {few_shot_examples}\n",
    "\n",
    "# Query: {query_str}\n",
    "# Answer: \\\n",
    "# \"\"\"\n",
    "\n",
    "# qa_prompt_tmpl = PromptTemplate(\n",
    "#     qa_prompt_tmpl_str,\n",
    "#     function_mappings={\"few_shot_examples\": few_shot_examples_fn},\n",
    "# )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedinbot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
