{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pdfplumber\n",
    "import json\n",
    "\n",
    "# Initialize the OpenAI client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load API key from secrets file\n",
    "secrets_file = r\"C:\\Users\\localadmin\\Desktop\\myprojects\\linkedinsucks\\secrets.json\"\n",
    "with open(secrets_file, 'r') as file:\n",
    "    data = json.load(file)\n",
    "OPENAI_API_KEY = data[\"OPENAI_KEY\"]\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Function to extract text from a PDF file using pdfplumber\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = r\"C:\\Users\\localadmin\\Desktop\\myprojects\\linkedinsucks\\linkedinsucks\\mydata\\resume_july_2024_master.pdf\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "document_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# # Print the extracted text to verify\n",
    "# print(\"Extracted Text:\\n\", document_text)\n",
    "\n",
    "# Define the prompt with instructions to extract key details\n",
    "prompt = f\"\"\"\n",
    "Please elaborate on the following resume with detailed descriptions for each section. \n",
    "For each experience and project, add a section called 'Skills and Tech Stack Used' and list all the technologies, programming languages, IDEs, frameworks, and skills used.\n",
    " Begin with the name, email address, phone number, LinkedIn, and GitHub links. \n",
    " Ensure the final document is well-structured and includes all relevant details.\n",
    " Generate the output in plain detailed text. \n",
    "\n",
    " \n",
    "Extract the following details from the resume and the output should begin with Name:\n",
    "- Name (The name is at the beginning of the resume, it is usually the first text in the resume. look carefully for the name, this information is important)\n",
    "- Email Address\n",
    "- Phone Number\n",
    "- LinkedIn\n",
    "- GitHub\n",
    "\n",
    "Resume:\n",
    "{document_text}\n",
    "\"\"\"\n",
    "\n",
    "# Call the GPT-4 API\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Get the response text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to elaborated_resume.txt\n"
     ]
    }
   ],
   "source": [
    "response_text = str(response.choices[0].message.content)\n",
    "# Save the response to a text file\n",
    "output_file_path = r\"C:\\Users\\localadmin\\Desktop\\myprojects\\linkedinsucks\\linkedinsucks\\mydata\\elaborated_resume.txt\"\n",
    "with open(output_file_path, \"w\") as output_file:\n",
    "    output_file.write(response_text)\n",
    "\n",
    "print(f\"Output saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Ganesh Raj K\\nEmail Address: ganesh_012@outlook.com\\nPhone Number: 848 313 8525\\nLinkedIn: linkedin.com/in/ganeshrajk\\nGitHub: github.com/ganeshraj-k\\n\\nEXPERIENCE:\\n\\n1. Rutgers UCM, Data Analyst\\n   Duration: Feb 2023 – Present\\n   Location: New Brunswick\\n   Description:\\n   - Harnessing learning data from canvas Api to predict new student course outcomes/CGPA grade using a classification model with a 78 percent success rate in classifying new students.\\n   - The dataset consisted of 12 diverse features including demographic academic, behavioral, parent participation for the predictive modeling.\\n   - Extraction of data using canvas Api, preprocessing using python, and using ANN for the classification model.\\n   Skills and Tech Stack Used: Python, Canvas API, ANN (Artificial Neural Networks)\\n\\n2. Deloitte Consulting, Data Analyst\\n   Duration: June 2019 – Jan 2022\\n   Location: Bengaluru\\n\\n   Medical Data NER:\\n   - Enhanced query speed for a medical record database with over 2 million records.\\n   - Used Amazon Comprehend with Python to perform Named Entity Recognition on the DynamoDB dataset.\\n   - Adding recognized entities as tags using AWS Glue for the ETL process.\\n   Skills and Tech Stack Used: Python, Amazon Comprehend, AWS Glue, DynamoDB\\n\\n   Restaurant Chain:\\n   - Categorized restaurant patrons based on their dining preferences from survey data using K-Means clustering.\\n   - Generated detailed Tableau visualizations to correlate the cluster results with their risk and safety behaviors.\\n   - The analysis helped strategize marketing along with the promotion of safety protocols, which led to a 74% increase in take away orders the next quarter.\\n   Skills and Tech Stack Used: K-Means Clustering, Tableau\\n\\n   Banking:\\n   - Mitigated the lockdown-induced customer churn by constructing a multivariate logistic regression model to Identify churn-prone customers and key contributing factors.\\n   - Performed EDA using matplotlib and communicated results to stakeholders, which helped them target marketing accordingly resulting in a 30% churn reduction in the next quarter.\\n   Skills and Tech Stack Used: Multivariate Logistic Regression, Matplotlib\\n\\n   Geospatial Intelligence:\\n   - Addressed the challenge of manually identifying docked vessels by developing an object detection system using Mask R-CNN and OpenCV for change detection in Python.\\n   - Accessed high-definition GIS satellite imagery from the Sentinel API in Python and dehazed the images for better results.\\n   - Extracted the geolocation data of the detected objects using QGIS.\\n   - Automated the entire process using AWS Lambda and CloudWatch, saving over $100k in labor costs.\\n   Skills and Tech Stack Used: Mask R-CNN, OpenCV, Python, Sentinel API, QGIS, AWS Lambda, AWS CloudWatch\\n\\n3. MAQ Software, Data Engineer\\n   Duration: May 2018 – July 2018\\n   Location: Hyderabad\\n   Description:\\n   - Established an ETL pipeline using SQL Server Management Studio and SSIS, consolidating three large Azure data marts with over 2 million records into one. Developed triggers and stored procedures in place to identify inconsistencies during the transfer and maintain data integrity.\\n   Skills and Tech Stack Used: SQL Server Management Studio, SSIS, Azure\\n\\nPROJECTS:\\n\\n1. Chatbot with a personality: [github]\\n   - Built a generative AI (Gen AI) model chatbot to replicate Chandler Bing’s dialogue style from “Friends,” utilizing an extensive dataset of 8,700 dialogues.\\n   - The model, featuring a seq2seq with 2-layer LSTM network with a dropout layer, achieved a BLEU score of 0.63.\\n   Skills and Tech Stack Used: Generative AI, seq2seq, LSTM, Python\\n\\n2. Twitter Search: [github]\\n   - Designed a web application with a local cache of 200 trending tweets, leveraging a combination of Postgres (relational) and MongoDB (non-relational) to query a dataset of about 120,000 tweets from 13,000 users.\\n   - Applied NLP techniques for efficient search, including synonym search and Levenshtein distance, and managed API requests with Flask\\n   Skills and Tech Stack Used: Postgres, MongoDB, NLP, Flask, Python\\n\\n3. 2024 Travelers Insurance Analytics University Contest:\\n   - Conducted Tweedie regression on a zero-inflated dataset of over 29,000 records, fine-tuned parameters using grid search, and assessed model efficacy with the Gini index. This systematic approach secured a third-place finish among 200+ teams.\\n   Skills and Tech Stack Used: Tweedie Regression, Grid Search, Gini Index, Python\\n\\nEDUCATION:\\n- MS in Data Science, Rutgers University, Completion: May 2024\\n- BTech in Computer Science and Engineering, Indian Institute of Technology Indore, Completion: May 2019\\n\\nSKILLS:\\n- Programming Languages: Python, R\\n- Machine Learning Libraries and Frameworks: PyTorch, scikit-learn, pandas, numpy\\n- Cloud: AWS, DynamoDB, Glue, EC2, SageMaker, IAM, S3\\n- Office: Excel, PowerPoint, VBA\\n- Data Visualization: Tableau, Matplotlib, Seaborn, Plotly\\n\\nCERTIFICATIONS:\\n- AWS Machine Learning Specialist\\n- AWS Cloud Practitioner'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedinbot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
